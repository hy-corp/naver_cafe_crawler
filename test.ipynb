{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구글시트 연결 완료\n",
      "로그인을 시도합니다...\n",
      "로그인 성공! 쿠키를 추출합니다...\n",
      "카페 로물콘에 지정된 게시판이 없으므로 '전체글'을 수집합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 17/180 [00:08<01:08,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 에러 (게시글 245216): 403 Client Error: Forbidden for url: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245216?query=&menuId=0&useCafeId=true&requestFrom=A / URL: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245216?query=&menuId=0&useCafeId=true&requestFrom=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 38/180 [00:18<00:57,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 에러 (게시글 245191): 403 Client Error: Forbidden for url: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245191?query=&menuId=0&useCafeId=true&requestFrom=A / URL: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245191?query=&menuId=0&useCafeId=true&requestFrom=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 54/180 [00:25<00:49,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 에러 (게시글 245175): 403 Client Error: Forbidden for url: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245175?query=&menuId=0&useCafeId=true&requestFrom=A / URL: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245175?query=&menuId=0&useCafeId=true&requestFrom=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 62/180 [00:29<00:45,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 에러 (게시글 245166): 403 Client Error: Forbidden for url: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245166?query=&menuId=0&useCafeId=true&requestFrom=A / URL: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245166?query=&menuId=0&useCafeId=true&requestFrom=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 74/180 [00:34<00:37,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 에러 (게시글 245154): 403 Client Error: Forbidden for url: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245154?query=&menuId=0&useCafeId=true&requestFrom=A / URL: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245154?query=&menuId=0&useCafeId=true&requestFrom=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 92/180 [00:43<00:37,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요청 에러 (게시글 245133): 403 Client Error: Forbidden for url: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245133?query=&menuId=0&useCafeId=true&requestFrom=A / URL: https://article.cafe.naver.com/gw/v3/cafes/28699715/articles/245133?query=&menuId=0&useCafeId=true&requestFrom=A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [01:25<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[로물콘] '전체글' 게시판에서 총 174개 게시글 수집 완료.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "총 174개의 새 데이터를 구글시트에 성공적으로 추가했습니다.\n",
      "구글시트 업로드 완료 (수집실패 갯수 : 6개)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread\n",
    "\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "from cookie import get_naver_cookies\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "'''\n",
    "fixed_day와 cafes_to_scrape(카페id), boards_to_scrape(게시판 이름)을 지정하면 해당 정보 크롤링\n",
    "\n",
    "'''\n",
    "# 수집할 날짜 지정\n",
    "today = datetime.now()\n",
    "fixed_day = datetime(2025, 9, 16)\n",
    "\n",
    "# 크롤링할 카페 이름과 ID - \"카페이름\": 12345678\n",
    "cafes_to_scrape = {\n",
    "    \"로물콘\": 28699715,\n",
    "}\n",
    "\n",
    "# 크롤링할 카페ID와 게시판 이름 - 카페ID: ['게시판1', '게시판2']\n",
    "boards_to_scrape = {\n",
    "    28699715: [],\n",
    "}\n",
    "\n",
    "# 값 담을 값들 정의\n",
    "board_num_dict = {}      # 게시판 {\"게시판이름\":\"게시판id\"}\n",
    "error_link = []          # 크롤링 실패한 링크\n",
    "final_list_of_dicts = [] # 크롤링 담길 리스트\n",
    "\n",
    "\n",
    "\n",
    "# 구글시트 url, gcp 인증키\n",
    "googlesheet_url = \"https://docs.google.com/spreadsheets/d/1sGCTNk_arqszCQgEeZ6yFdfh75qYSjqPkIx7gaZl5ho/edit?gid=0#gid=0\"\n",
    "gcp_api_key = \"navercafe-crawler-aafd3370cb81.json\"\n",
    "\n",
    "def google_sheet(sheet =\"원본데이터\", url=googlesheet_url, key = gcp_api_key):\n",
    "\n",
    "    '''\n",
    "    구글시트에서 시트 정보를 가져오는 코드입니다.\n",
    "    google_sheet(\"시트이름\", \"주소\")를 가져오시면 해당 시트로 연겯룁니다.\n",
    "    '''\n",
    "\n",
    "    # 인증 및 스프레드시트 설정\n",
    "    scope = ['https://spreadsheets.google.com/feeds',\n",
    "            'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    credential = ServiceAccountCredentials.from_json_keyfile_name(key, scope)\n",
    "    gc = gspread.authorize(credential)\n",
    "\n",
    "    # 스프레드시트 URL\n",
    "    doc = gc.open_by_url(url)\n",
    "\n",
    "    # 시트선택\n",
    "    sheet = doc.worksheet(sheet)\n",
    "\n",
    "    return sheet\n",
    "\n",
    "\n",
    "\n",
    "# 구글시트 원본시트에 연결\n",
    "raw_sheet = google_sheet() \n",
    "# 구글시트 원본시트 데이터 json으로 가져오기\n",
    "sheet_extract_data = raw_sheet.get_all_records() \n",
    "# (카페, 게시글번호)형태로 이미 크롤링한 값 필터 생성\n",
    "existing_posts = {(row.get('카페'), str(row.get('게시글번호'))) for row in sheet_extract_data}\n",
    "print(\"구글시트 연결 완료\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 카페의 게시판 이름을 불러오기 위한 헤더\n",
    "my_cookie = get_naver_cookies()\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"cookie\": my_cookie,\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://cafe.naver.com\",\n",
    "    \"referer\": \"https://cafe.naver.com/\",\n",
    "    \"x-cafe-product\": \"pc\"\n",
    "}\n",
    "\n",
    "# -----------------------카페별 게시판ID를 수집합니다.-------------------------------------\n",
    "for cafe_name, cafe_id in cafes_to_scrape.items():\n",
    "    \n",
    "    url = f\"https://apis.naver.com/cafe-web/cafe-cafemain-api/v1.0/cafes/{cafe_id}/menus\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        \n",
    "        all_menus = data.get('result', {}).get('menus', [])\n",
    "        all_menus.extend(data.get('result', {}).get('linkMenus', []))\n",
    "        \n",
    "        # 해당 카페에서 원하는 게시판 이름 리스트를 가져옵니다.\n",
    "        target_board_names = boards_to_scrape.get(cafe_id, [])\n",
    "        board_ids = [] # 카페 id가 담길 리스트\n",
    "\n",
    "        if not all_menus:\n",
    "            print(\"메뉴 목록을 찾을 수 없습니다.\")\n",
    "        elif not target_board_names:\n",
    "    # 리스트가 비어있으면 board_ids에 0만 추가\n",
    "            board_ids.append(0)\n",
    "            print(f\"카페 {cafe_name}에 지정된 게시판이 없으므로 '전체글'을 수집합니다.\")\n",
    "        else:\n",
    "            for menu in all_menus:\n",
    "                menu_id = menu.get('menuId')\n",
    "                menu_name = menu.get('name')\n",
    "\n",
    "                board_num_dict[menu_name] = menu_id\n",
    "\n",
    "                # 게시판 이름에 맞는 카페 id 추출\n",
    "                if menu_name in target_board_names:\n",
    "                    board_ids.append(menu_id)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[{cafe_name}] URL 요청 중 오류 발생: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[{cafe_name}] JSON 파싱 오류: {e}\")\n",
    "\n",
    "\n",
    "    # --------------------게시판에서 게시글의 게시번호를 수집하여 article_id_list에 담습니다.-----------------------------\n",
    "    for ids in board_ids:\n",
    "        article_id_list = []  # 게시물 id를 담을 리스트\n",
    "        should_stop = False   # 페이지 반복을 멈추기 위한 플래그 변수 추가\n",
    "\n",
    "        # 페이지 번호는 임의로 큰 값 5000 지정 (지정 날짜 도달시 자동으로 멈추는 코드가 존재하므로)\n",
    "        for page in range(1,5000):\n",
    "            menu_url = f\"https://apis.naver.com/cafe-web/cafe-boardlist-api/v1/cafes/{cafe_id}/menus/{ids}/articles?page={page}&sortBy=TIME\"\n",
    "        \n",
    "            try:\n",
    "                response = requests.get(menu_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                list_per_post = len(response.json()['result']['articleList']) # 목록의 게시글 수\n",
    "\n",
    "                for num in range(list_per_post):\n",
    "                    post_timestamp = response.json()['result']['articleList'][num]['item']['writeDateTimestamp']\n",
    "                    post_time =  post_timestamp/1000 # 밀리초를 초 단위로 변환\n",
    "                    post_day = datetime.fromtimestamp(post_time)\n",
    "\n",
    "                    # 지정 날짜가 지나면 목록 크롤링을 멈춘다.\n",
    "                    if fixed_day > post_day:\n",
    "                        should_stop = True                        \n",
    "                    else:\n",
    "                        article_id = response.json().get('result')['articleList'][num]['item']['articleId']\n",
    "\n",
    "                        if (cafe_name, str(article_id)) in existing_posts:\n",
    "                            break\n",
    "                        else:\n",
    "                            article_id_list.append(article_id)\n",
    "\n",
    "                # 지정 날짜가 지났거나 중복 게시글 발견시 해당 게시판 작업을 멈춘다\n",
    "                if should_stop:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        \n",
    "        # -------------- 수집된 게시글의 작성날짜, 제목, 본문, 댓글을 수집합니다. -------------------------\n",
    "        for article in tqdm(article_id_list):\n",
    "            article_url = f\"https://article.cafe.naver.com/gw/v3/cafes/{cafe_id}/articles/{article}?query=&menuId={ids}&useCafeId=true&requestFrom=A\"\n",
    "\n",
    "            try:\n",
    "                response = requests.get(article_url, headers=headers)\n",
    "                response.raise_for_status() \n",
    "                data = response.json()\n",
    "\n",
    "                # --- 에러 처리 로직 ---\n",
    "                # JSON 응답의 최상위에 'errorCode' 키가 있는지 확인\n",
    "                if data.get('errorCode') == '0004':\n",
    "                    error_link.append(article_url)\n",
    "                    print(f\"게시글 ID {article}: 로그인 에러 발생, 건너뜁니다.\")\n",
    "                    continue # 다음 게시글로 넘어감\n",
    "\n",
    "                # --- 게시글 데이터 추출 ---\n",
    "                result = data.get('result', {})\n",
    "                article_data = result.get('article', {})\n",
    "                \n",
    "                # 게시글 제목 추출\n",
    "                title = article_data.get('subject', '제목 없음')\n",
    "\n",
    "                html_content = article_data.get('contentHtml')\n",
    "                if not html_content:\n",
    "                    html_content = result.get('scrap', {}).get('contentHtml', '')\n",
    "                    \n",
    "                # HTML에서 텍스트만 추출\n",
    "                article_content = BeautifulSoup(html_content, 'html.parser').get_text(strip=True, separator='\\n')\n",
    "\n",
    "                # 댓글 추출\n",
    "                comments = result.get('comments', {}).get('items', [])\n",
    "                comments_list = [BeautifulSoup(comment.get('content', ''), 'html.parser').get_text(strip=True, separator='\\n')\n",
    "                                 for comment in comments]\n",
    "                \n",
    "\n",
    "                posting_time = response.json()['result']['article']['writeDate']/1000 # 밀리초를 초 단위로 변환\n",
    "                posting_date = datetime.fromtimestamp(posting_time).strftime(\"%Y-%m-%d\")\n",
    "                \n",
    "\n",
    "                # --- 데이터 저장 ---\n",
    "                # 게시글 하나의 정보를 딕셔너리에 담아 리스트에 추가합니다.\n",
    "                comments_str = \"\\n\".join(comments_list)\n",
    "\n",
    "                article_info = {\n",
    "                    '카페' : cafe_name,\n",
    "                    '날짜' : posting_date,\n",
    "                    '제목': title,\n",
    "                    '본문': article_content,\n",
    "                    '댓글(줄당1개)': comments_str, # 리스트 대신 문자열을 저장\n",
    "                    '게시글번호' : article  # 인덱스\n",
    "                }\n",
    "                final_list_of_dicts.append(article_info)\n",
    "\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"요청 에러 (게시글 {article}): {e} / URL: {article_url}\")\n",
    "                error_link.append(article_url)\n",
    "                continue\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError (게시글 {article}): JSON 아님. 응답 앞부분 ↓\")\n",
    "                print(response.text[:200])  # HTML일 가능성 ↑\n",
    "                continue\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError (게시글 {article}): {e}, data keys: {list(data.keys())}\")\n",
    "                continue\n",
    "    \n",
    "\n",
    "        # ----------------------- 게시판 ID(ids)에 해당하는 이름을 찾습니다. ----------------------------\n",
    "        if ids == 0:\n",
    "            board_name = '전체글'  # ID가 0일 때 사용할 게시판 이름\n",
    "        else:\n",
    "            # 이 코드가 제대로 작동하려면 이전에 board_num_dict가 생성되어 있어야 합니다.\n",
    "            board_name = [name for name, id in board_num_dict.items() if id == ids][0]\n",
    "\n",
    "\n",
    "        # 원하는 문구를 출력합니다.\n",
    "        print(f\"[{cafe_name}] '{board_name}' 게시판에서 총 {len(article_id_list)-len(error_link)}개 게시글 수집 완료.\")\n",
    "        print('')\n",
    "        print('-'*100)\n",
    "\n",
    "if final_list_of_dicts:\n",
    "    cafe_data = pd.DataFrame(final_list_of_dicts)\n",
    "    cafe_data = cafe_data.sort_values(by='날짜', ascending=True)\n",
    "    raw_sheet.update([cafe_data.columns.tolist()])\n",
    "    values_to_append = cafe_data.values.tolist()\n",
    "    raw_sheet.append_rows(values_to_append, value_input_option='USER_ENTERED')\n",
    "    print(f\"총 {len(values_to_append)}개의 새 데이터를 구글시트에 성공적으로 추가했습니다.\")\n",
    "\n",
    "    print(f'구글시트 업로드 완료 (수집실패 갯수 : {len(error_link)}개)')\n",
    "else:\n",
    "    print(f\"수집할 데이터가 없습니다 (수집실패 갯수 : {len(error_link)}개)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구글시트 연결 완료\n",
      "로그인을 시도합니다...\n",
      "로그인 성공! 쿠키를 추출합니다...\n",
      "카페 로물콘에 지정된 게시판이 없으므로 '전체글'을 수집합니다.\n",
      "[로물콘] 메뉴 0 상세 1건 병렬 수집 시작...\n",
      "[로물콘] '전체글' 게시판에서 총 1개 게시글 수집 누적.\n",
      "총 1개의 새 데이터를 구글시트에 성공적으로 추가했습니다.\n",
      "구글시트 업로드 완료 (수집실패 갯수 : 0개)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread\n",
    "\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "from cookie import get_naver_cookies\n",
    "\n",
    "# === 추가: 비동기 ===\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import asyncio\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "\n",
    "def run_async(coro):\n",
    "    \"\"\"\n",
    "    이미 실행 중인 이벤트 루프가 있으면 그 위에서 실행하고,\n",
    "    없으면 asyncio.run으로 실행한다.\n",
    "    (Jupyter 등에서 안전하게 동작)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = None\n",
    "\n",
    "    if loop and loop.is_running():\n",
    "        # 노트북/스트림릿 등: 재진입 허용\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "    else:\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "\n",
    "'''\n",
    "fixed_day와 cafes_to_scrape(카페id), boards_to_scrape(게시판 이름)을 지정하면 해당 정보 크롤링\n",
    "'''\n",
    "# 수집할 날짜 지정\n",
    "today = datetime.now()\n",
    "fixed_day = datetime(2025, 9, 15)\n",
    "\n",
    "# 크롤링할 카페 이름과 ID - \"카페이름\": 12345678\n",
    "cafes_to_scrape = {\n",
    "    \"로물콘\": 28699715,\n",
    "}\n",
    "\n",
    "# 크롤링할 카페ID와 게시판 이름 - 카페ID: ['게시판1', '게시판2']\n",
    "boards_to_scrape = {\n",
    "    28699715: [],\n",
    "}\n",
    "\n",
    "# 값 담을 값들 정의\n",
    "board_num_dict = {}      # 게시판 {\"게시판이름\":\"게시판id\"}\n",
    "error_link = []          # 크롤링 실패한 링크\n",
    "final_list_of_dicts = [] # 크롤링 담길 리스트\n",
    "\n",
    "# 구글시트 url, gcp 인증키\n",
    "googlesheet_url = \"https://docs.google.com/spreadsheets/d/1sGCTNk_arqszCQgEeZ6yFdfh75qYSjqPkIx7gaZl5ho/edit?gid=0#gid=0\"\n",
    "gcp_api_key = \"navercafe-crawler-aafd3370cb81.json\"\n",
    "\n",
    "def google_sheet(sheet =\"원본데이터\", url=googlesheet_url, key = gcp_api_key):\n",
    "    '''\n",
    "    구글시트에서 시트 정보를 가져오는 코드입니다.\n",
    "    google_sheet(\"시트이름\", \"주소\")를 가져오시면 해당 시트로 연겯룁니다.\n",
    "    '''\n",
    "    scope = ['https://spreadsheets.google.com/feeds',\n",
    "            'https://www.googleapis.com/auth/drive']\n",
    "    credential = ServiceAccountCredentials.from_json_keyfile_name(key, scope)\n",
    "    gc = gspread.authorize(credential)\n",
    "    doc = gc.open_by_url(url)\n",
    "    sheet = doc.worksheet(sheet)\n",
    "    return sheet\n",
    "\n",
    "# 구글시트 원본시트에 연결\n",
    "raw_sheet = google_sheet()\n",
    "# 구글시트 원본시트 데이터 json으로 가져오기\n",
    "sheet_extract_data = raw_sheet.get_all_records()\n",
    "# (카페, 게시글번호)형태로 이미 크롤링한 값 필터 생성\n",
    "existing_posts = {(row.get('카페'), str(row.get('게시글번호'))) for row in sheet_extract_data}\n",
    "print(\"구글시트 연결 완료\")\n",
    "\n",
    "# 카페의 게시판 이름을 불러오기 위한 헤더\n",
    "my_cookie = get_naver_cookies()\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"cookie\": my_cookie,\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"origin\": \"https://cafe.naver.com\",\n",
    "    \"referer\": \"https://cafe.naver.com/\",\n",
    "    \"x-cafe-product\": \"pc\"\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# (추가) 비동기 헬퍼들\n",
    "# =========================\n",
    "async def fetch_article_json(session: aiohttp.ClientSession, url: str) -> Tuple[Optional[Dict[str, Any]], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    게시글 상세 JSON 요청.\n",
    "    return: (data, err_reason, text_head_if_decode_err)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with session.get(url, timeout=aiohttp.ClientTimeout(total=20)) as resp:\n",
    "            if resp.status >= 400:\n",
    "                return None, f\"HTTP {resp.status}\", None\n",
    "            # 헤더가 text/html이어도 JSON일 수 있어 content_type=None로 파싱 시도\n",
    "            try:\n",
    "                data = await resp.json(content_type=None)\n",
    "                if isinstance(data, dict):\n",
    "                    return data, None, None\n",
    "                return None, \"JSONNotDict\", None\n",
    "            except Exception:\n",
    "                head = (await resp.text())[:200]\n",
    "                return None, \"JSONDecodeError\", head\n",
    "    except asyncio.TimeoutError:\n",
    "        return None, \"Timeout\", None\n",
    "    except aiohttp.ClientError as ce:\n",
    "        return None, f\"ClientError: {ce}\", None\n",
    "    except Exception as e:\n",
    "        return None, f\"UnknownError: {e}\", None\n",
    "\n",
    "def parse_article_data(cafe_name: str, cafe_id: int, article: int, menu_id: int, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    기존 동기 코드와 동일한 파싱 로직을 유지\n",
    "    \"\"\"\n",
    "    # --- 에러 처리 로직 ---\n",
    "    if data.get('errorCode') == '0004':\n",
    "        raise ValueError(\"LoginError(0004)\")\n",
    "\n",
    "    result = data.get('result', {})\n",
    "    article_data = result.get('article', {})\n",
    "\n",
    "    # 제목\n",
    "    title = article_data.get('subject', '제목 없음')\n",
    "\n",
    "    # 본문 HTML→텍스트\n",
    "    html_content = article_data.get('contentHtml')\n",
    "    if not html_content:\n",
    "        html_content = result.get('scrap', {}).get('contentHtml', '')\n",
    "    article_content = BeautifulSoup(html_content, 'html.parser').get_text(strip=True, separator='\\n')\n",
    "\n",
    "    # 댓글\n",
    "    comments = result.get('comments', {}).get('items', [])\n",
    "    comments_list = [BeautifulSoup(comment.get('content', ''), 'html.parser').get_text(strip=True, separator='\\n')\n",
    "                     for comment in comments]\n",
    "\n",
    "    # 날짜\n",
    "    posting_time = article_data.get('writeDate')\n",
    "    if posting_time:\n",
    "        posting_date = datetime.fromtimestamp(posting_time/1000).strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        posting_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return {\n",
    "        '카페': cafe_name,\n",
    "        '날짜': posting_date,\n",
    "        '제목': title,\n",
    "        '본문': article_content,\n",
    "        '댓글(줄당1개)': \"\\n\".join(comments_list),\n",
    "        '게시글번호': article\n",
    "    }\n",
    "\n",
    "async def fetch_articles_concurrently(cafe_name: str, cafe_id: int, menu_id: int,\n",
    "                                      article_id_list: List[int], base_headers: Dict[str, str],\n",
    "                                      concurrency: int = 25):\n",
    "    \"\"\"\n",
    "    게시글 상세들을 비동기로 병렬 수집하여 final_list_of_dicts, error_link를 갱신\n",
    "    (전역 리스트 사용 → 기존 코드와의 호환 유지)\n",
    "    \"\"\"\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    # 게시판별 referer를 맞춰주면 안정적\n",
    "    h = dict(base_headers)\n",
    "    h[\"referer\"] = f\"https://cafe.naver.com/f-e/cafes/{cafe_id}/menus/{menu_id}\"\n",
    "\n",
    "    async with aiohttp.ClientSession(headers=h, cookie_jar=aiohttp.CookieJar(unsafe=True)) as session:\n",
    "\n",
    "        async def _one(article: int):\n",
    "            article_url = f\"https://article.cafe.naver.com/gw/v3/cafes/{cafe_id}/articles/{article}?query=&menuId={menu_id}&useCafeId=true&requestFrom=A\"\n",
    "            async with sem:\n",
    "                data, err, head = await fetch_article_json(session, article_url)\n",
    "                if err:\n",
    "                    if err == \"JSONDecodeError\" and head:\n",
    "                        print(f\"JSONDecodeError (게시글 {article}): JSON 아님. 응답 앞부분 ↓\\n{head}\")\n",
    "                    else:\n",
    "                        print(f\"요청 에러 (게시글 {article}): {err} / URL: {article_url}\")\n",
    "                    error_link.append(article_url)\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    parsed = parse_article_data(cafe_name, cafe_id, article, menu_id, data)\n",
    "                    final_list_of_dicts.append(parsed)\n",
    "                except ValueError as ve:\n",
    "                    # LoginError(0004) 등\n",
    "                    print(f\"게시글 ID {article}: {ve}, 건너뜁니다.\")\n",
    "                    error_link.append(article_url)\n",
    "                except KeyError as ke:\n",
    "                    print(f\"KeyError (게시글 {article}): {ke}, data keys: {list(data.keys())}\")\n",
    "                    error_link.append(article_url)\n",
    "                except Exception as e:\n",
    "                    print(f\"ParseError (게시글 {article}): {e}\")\n",
    "                    error_link.append(article_url)\n",
    "\n",
    "        # 진행바는 동기 tqdm과 충돌될 수 있어 간단히 gather만\n",
    "        await asyncio.gather(*[_one(a) for a in article_id_list])\n",
    "\n",
    "# -----------------------카페별 게시판ID를 수집합니다.-------------------------------------\n",
    "for cafe_name, cafe_id in cafes_to_scrape.items():\n",
    "    url = f\"https://apis.naver.com/cafe-web/cafe-cafemain-api/v1.0/cafes/{cafe_id}/menus\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        all_menus = data.get('result', {}).get('menus', [])\n",
    "        all_menus.extend(data.get('result', {}).get('linkMenus', []))\n",
    "\n",
    "        # 해당 카페에서 원하는 게시판 이름 리스트를 가져옵니다.\n",
    "        target_board_names = boards_to_scrape.get(cafe_id, [])\n",
    "        board_ids = [] # 카페 id가 담길 리스트\n",
    "\n",
    "        if not all_menus:\n",
    "            print(\"메뉴 목록을 찾을 수 없습니다.\")\n",
    "        elif not target_board_names:\n",
    "            # 리스트가 비어있으면 board_ids에 0만 추가\n",
    "            board_ids.append(0)\n",
    "            print(f\"카페 {cafe_name}에 지정된 게시판이 없으므로 '전체글'을 수집합니다.\")\n",
    "        else:\n",
    "            for menu in all_menus:\n",
    "                menu_id = menu.get('menuId')\n",
    "                menu_name = menu.get('name')\n",
    "\n",
    "                board_num_dict[menu_name] = menu_id\n",
    "\n",
    "                # 게시판 이름에 맞는 카페 id 추출\n",
    "                if menu_name in target_board_names:\n",
    "                    board_ids.append(menu_id)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[{cafe_name}] URL 요청 중 오류 발생: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[{cafe_name}] JSON 파싱 오류: {e}\")\n",
    "\n",
    "    # --------------------게시판에서 게시글의 게시번호를 수집하여 article_id_list에 담습니다.-----------------------------\n",
    "    for ids in board_ids:\n",
    "        article_id_list = []  # 게시물 id를 담을 리스트\n",
    "        should_stop = False   # 페이지 반복을 멈추기 위한 플래그 변수 추가\n",
    "\n",
    "        # 페이지 번호는 임의로 큰 값 5000 지정 (지정 날짜 도달시 자동으로 멈추는 코드가 존재하므로)\n",
    "        for page in range(1,5000):\n",
    "            menu_url = f\"https://apis.naver.com/cafe-web/cafe-boardlist-api/v1/cafes/{cafe_id}/menus/{ids}/articles?page={page}&sortBy=TIME\"\n",
    "\n",
    "            try:\n",
    "                response = requests.get(menu_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                list_per_post = len(response.json()['result']['articleList']) # 목록의 게시글 수\n",
    "                for num in range(list_per_post):\n",
    "                    post_timestamp = response.json()['result']['articleList'][num]['item']['writeDateTimestamp']\n",
    "                    post_time =  post_timestamp/1000 # 밀리초를 초 단위로 변환\n",
    "                    post_day = datetime.fromtimestamp(post_time)\n",
    "\n",
    "                    # 지정 날짜가 지나면 목록 크롤링을 멈춘다.\n",
    "                    if fixed_day > post_day:\n",
    "                        should_stop = True\n",
    "                        break\n",
    "                    else:\n",
    "                        article_id = response.json().get('result')['articleList'][num]['item']['articleId']\n",
    "\n",
    "                        if (cafe_name, str(article_id)) in existing_posts:\n",
    "                            should_stop = True\n",
    "                            break\n",
    "                        else:\n",
    "                            article_id_list.append(article_id)\n",
    "\n",
    "                # 지정 날짜가 지났거나 중복 게시글 발견시 해당 게시판 작업을 멈춘다\n",
    "                if should_stop:\n",
    "                    break\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "        # -------------- (변경) 상세 수집: 비동기 병렬 -------------------------\n",
    "        if article_id_list:\n",
    "            print(f\"[{cafe_name}] 메뉴 {ids} 상세 {len(article_id_list)}건 병렬 수집 시작...\")\n",
    "            run_async(fetch_articles_concurrently(\n",
    "                cafe_name=cafe_name,\n",
    "                cafe_id=cafe_id,\n",
    "                menu_id=ids,\n",
    "                article_id_list=article_id_list,\n",
    "                base_headers=headers,\n",
    "                concurrency=30,\n",
    "            ))\n",
    "\n",
    "        # ----------------------- 게시판 ID(ids)에 해당하는 이름을 찾습니다. ----------------------------\n",
    "        if ids == 0:\n",
    "            board_name = '전체글'  # ID가 0일 때 사용할 게시판 이름\n",
    "        else:\n",
    "            board_name = [name for name, id in board_num_dict.items() if id == ids][0] if board_num_dict else f\"menu:{ids}\"\n",
    "\n",
    "        # 원하는 문구를 출력합니다.\n",
    "        succ_cnt = sum(1 for r in final_list_of_dicts if (r['카페']==cafe_name))\n",
    "        print(f\"[{cafe_name}] '{board_name}' 게시판에서 총 {succ_cnt}개 게시글 수집 누적.\")\n",
    "\n",
    "# =========================\n",
    "# 업로드\n",
    "# =========================\n",
    "if final_list_of_dicts:\n",
    "    cafe_data = pd.DataFrame(final_list_of_dicts)\n",
    "    cafe_data = cafe_data.sort_values(by='날짜', ascending=False)\n",
    "    raw_sheet.update([cafe_data.columns.tolist()])\n",
    "    values_to_append = cafe_data.values.tolist()\n",
    "    raw_sheet.append_rows(values_to_append, value_input_option='USER_ENTERED')\n",
    "    print(f\"총 {len(values_to_append)}개의 새 데이터를 구글시트에 성공적으로 추가했습니다.\")\n",
    "    print(f'구글시트 업로드 완료 (수집실패 갯수 : {len(error_link)}개)')\n",
    "else:\n",
    "    print(f\"수집할 데이터가 없습니다 (수집실패 갯수 : {len(error_link)}개)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
